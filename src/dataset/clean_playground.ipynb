{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9686d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import utils\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "import os\n",
    "import nibabel as nib\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import skimage.transform as skTrans\n",
    "import cv2\n",
    "from numpy import logical_and as l_and, logical_not as l_not\n",
    "from scipy.spatial.distance import directed_hausdorff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26b8dcb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de65d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 4\n",
    "resize_shape = (144,144,144)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22c5449",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47560ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleToFixed(object):\n",
    "\n",
    "    def __init__(self, new_shape, interpolation=1, channels=4):\n",
    "        self.shape= new_shape\n",
    "        self.interpolation = interpolation\n",
    "        self.channels = channels\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # print('first shape', image.shape)\n",
    "        if image is not None: # (some patients don't have segmentations)\n",
    "            if self.channels == 1:\n",
    "                short_shape = (self.shape[1], self.shape[2], self.shape[3])\n",
    "                image = skTrans.resize(image, short_shape, order=self.interpolation, preserve_range=True)  #\n",
    "                image = image.reshape(self.shape)\n",
    "            else:\n",
    "                image = skTrans.resize(image, self.shape, order=self.interpolation, preserve_range=True)  #\n",
    "\n",
    "        # print('second shape', image.shape)\n",
    "        # print()\n",
    "        return image\n",
    "\n",
    "class RandomFlip(object):\n",
    "    \"\"\"Randomly flips (horizontally as well as vertically) the given PIL.Image with a probability of 0.5\n",
    "    \"\"\"\n",
    "    def __init__(self, prob_flip=0.5):\n",
    "        self.prob_flip= prob_flip\n",
    "    def __call__(self, image):\n",
    "\n",
    "        if random.random() < self.prob_flip:\n",
    "            flip_type = np.random.randint(0, 3) # flip across any 3D axis\n",
    "            image = np.flip(image, flip_type)\n",
    "        return image\n",
    "\n",
    "class ZeroChannel(object):\n",
    "    \"\"\"Randomly sets channel to zero the given PIL.Image with a probability of 0.25\n",
    "    \"\"\"\n",
    "    def __init__(self, prob_zero=0.25, channels=4):\n",
    "        self.prob_zero= prob_zero\n",
    "        self.channels = channels\n",
    "    def __call__(self, image):\n",
    "\n",
    "        if np.random.random() < self.prob_zero:\n",
    "            channel_to_zero = np.random.randint(0, self.channels) # flip across any 3D axis\n",
    "            zeros = np.zeros((image.shape[1], image.shape[2], image.shape[3]))\n",
    "            image[channel_to_zero, :, :, :] = zeros\n",
    "        return image\n",
    "\n",
    "class ZeroSprinkle(object):\n",
    "    def __init__(self, prob_zero=0.25, prob_true=0.5, channels=4):\n",
    "        self.prob_zero=prob_zero\n",
    "        self.prob_true=prob_true\n",
    "        self.channels=channels\n",
    "    def __call__(self, image):\n",
    "\n",
    "        if self.prob_true:\n",
    "            mask = np.random.rand(image.shape[0], image.shape[1], image.shape[2], image.shape[3])\n",
    "            mask[mask < self.prob_zero] = 0\n",
    "            mask[mask > 0] = 1\n",
    "            image = image*mask\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "class MinMaxNormalize(object):\n",
    "    \"\"\"Min-Max normalization\n",
    "    \"\"\"\n",
    "    def __call__(self, image):\n",
    "        def norm(im):\n",
    "            im = im.astype(np.float32)\n",
    "            min_v = np.min(im)\n",
    "            max_v = np.max(im)\n",
    "            im = (im - min_v)/(max_v - min_v)\n",
    "            return im\n",
    "        image = norm(image)\n",
    "        return image\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __init__(self, scale=1):\n",
    "        self.scale = scale\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if image is not None:\n",
    "            image = image.astype(np.float32)\n",
    "            image = image.reshape((image.shape[0], int(image.shape[1]/self.scale), int(image.shape[2]/self.scale), int(image.shape[3]/self.scale)))\n",
    "            image_tensor = torch.from_numpy(image)\n",
    "            return image_tensor\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    \"\"\"\n",
    "    Composes several transforms together.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image):\n",
    "        for i, t in enumerate(self.transforms):\n",
    "            image = t(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "761d535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# basic data augmentation\n",
    "prob_voxel_zero = 0 # 0.1\n",
    "prob_channel_zero = 0 # 0.5\n",
    "prob_true = 0 # 0.8\n",
    "randomflip = RandomFlip()\n",
    "\n",
    "# MRI transformations\n",
    "train_transformations = Compose([\n",
    "    MinMaxNormalize(),\n",
    "    ScaleToFixed((channels, resize_shape[0],resize_shape[1],resize_shape[2]),\n",
    "                          interpolation=1,\n",
    "                          channels=channels),\n",
    "    ZeroSprinkle(prob_zero=prob_voxel_zero, prob_true=prob_true),\n",
    "    ZeroChannel(prob_zero=prob_channel_zero),\n",
    "    randomflip,\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "# GT segmentation mask transformations\n",
    "\n",
    "seg_transformations = Compose([\n",
    "            ScaleToFixed((1, resize_shape[0],resize_shape[1],resize_shape[2]),\n",
    "                                      interpolation=0,\n",
    "                                      channels=1),\n",
    "            randomflip,\n",
    "            ToTensor(),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2edc92",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22e98a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bb_3D(img, pad=0):\n",
    "    '''\n",
    "    This function returns a tumor 3D bounding box using a segmentation mask\n",
    "    '''\n",
    "    xs = np.nonzero(np.sum(np.sum(img, axis=1), axis=1))\n",
    "    ys = np.nonzero(np.sum(np.sum(img, axis=0), axis=1))\n",
    "    zs = np.nonzero(np.sum(np.sum(img, axis=0), axis=0))\n",
    "    xmin, xmax = np.min(xs), np.max(xs)\n",
    "    ymin, ymax = np.min(ys), np.max(ys)\n",
    "    zmin, zmax = np.min(zs), np.max(zs)\n",
    "    bbox = (xmin-pad, ymin-pad, zmin-pad, xmax+pad, ymax+pad, zmax+pad)\n",
    "    return bbox\n",
    "\n",
    "def min_max(img):\n",
    "    '''\n",
    "    Min-max normalization\n",
    "    '''\n",
    "    return (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "def read_mri(mr_path_dict, pad=0):\n",
    "\n",
    "    image_shape = nib.load(mr_path_dict['flair']).get_fdata().shape\n",
    "    bb_seg = get_bb_3D(nib.load(mr_path_dict['flair']).get_fdata())\n",
    "    (xmin, ymin, zmin, xmax, ymax, zmax) = bb_seg\n",
    "\n",
    "    xmin = np.max([0, xmin-pad])\n",
    "    ymin = np.max([0, ymin-pad])\n",
    "    zmin = np.max([0, zmin-pad])\n",
    "\n",
    "    xmax = np.min([image_shape[0]-1, xmax+pad])\n",
    "    ymax = np.min([image_shape[1]-1, ymax+pad])\n",
    "    zmax = np.min([image_shape[2]-1, zmax+pad])\n",
    "\n",
    "\n",
    "    img_dict = {}\n",
    "    for key in ['flair', 't1', 't1ce', 't2', 'seg']:\n",
    "        img = nib.load(mr_path_dict[key])\n",
    "        img_data = img.get_fdata()\n",
    "        img_dict[key] = img_data[xmin:xmax, ymin:ymax, zmin:zmax]\n",
    "\n",
    "    stacked_img = np.stack([min_max(img_dict['flair']), min_max(img_dict['t1']),min_max(img_dict['t1ce']),min_max(img_dict['t2'])], axis=0)\n",
    "    return stacked_img, img_dict['seg']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "387e98b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_(image, seg, predicted=False):\n",
    "    #Overlay with Predicted\n",
    "    img = image[slice, :, :, :].squeeze()\n",
    "    img = utils.make_grid(img)\n",
    "    img = img.detach().cpu().numpy()\n",
    "    \n",
    "    print(img.shape)\n",
    "    \n",
    "    # plot images\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    img_list = [img[i].T for i in range(channels)] # 1 image per channel\n",
    "    plt.imshow(np.hstack(img_list), cmap='Greys_r')\n",
    "    \n",
    "    ## plot segmentation mask ##\n",
    "    seg_img = torch.tensor(pred[slice].squeeze())\n",
    "    if not predicted:\n",
    "        seg_img = torch.tensor(seg_img.numpy()[:, ::-1].copy()) #flip\n",
    "    seg_img = utils.make_grid(seg_img).detach().cpu().numpy()\n",
    "    \n",
    "    print(np.unique(seg_img))\n",
    "\n",
    "    plt.imshow(np.hstack([seg_img[0].T]), cmap='Greys_r', alpha=0.3)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9615fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(preds, targets, patient, tta=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    preds:\n",
    "        torch tensor of size 1*C*Z*Y*X\n",
    "    targets:\n",
    "        torch tensor of same shape\n",
    "    patient :\n",
    "        The patient ID\n",
    "    tta:\n",
    "        is tta performed for this run\n",
    "    \"\"\"\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    assert preds.shape == targets.shape, \"Preds and targets do not have the same size\"\n",
    "\n",
    "    labels = [\"ET\", \"TC\", \"WT\"]\n",
    "\n",
    "    metrics_list = []\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        metrics = dict(\n",
    "            patient_id=patient,\n",
    "            label=label,\n",
    "            tta=tta,\n",
    "        )\n",
    "\n",
    "        if np.sum(targets[i]) == 0:\n",
    "            print(f\"{label} not present for {patient}\")\n",
    "            sens = np.nan\n",
    "            dice = 1 if np.sum(preds[i]) == 0 else 0\n",
    "            tn = np.sum(l_and(l_not(preds[i]), l_not(targets[i])))\n",
    "            fp = np.sum(l_and(preds[i], l_not(targets[i])))\n",
    "            spec = tn / (tn + fp)\n",
    "            haussdorf_dist = np.nan\n",
    "\n",
    "        else:\n",
    "            preds_coords = np.argwhere(preds[i])\n",
    "            targets_coords = np.argwhere(targets[i])\n",
    "            haussdorf_dist = directed_hausdorff(preds_coords, targets_coords)[0]\n",
    "\n",
    "            tp = np.sum(l_and(preds[i], targets[i]))\n",
    "            tn = np.sum(l_and(l_not(preds[i]), l_not(targets[i])))\n",
    "            fp = np.sum(l_and(preds[i], l_not(targets[i])))\n",
    "            fn = np.sum(l_and(l_not(preds[i]), targets[i]))\n",
    "\n",
    "            sens = tp / (tp + fn)\n",
    "            spec = tn / (tn + fp)\n",
    "\n",
    "            dice = 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "        metrics[HAUSSDORF] = haussdorf_dist\n",
    "        metrics[DICE] = dice\n",
    "        metrics[SENS] = sens\n",
    "        metrics[SPEC] = spec\n",
    "        pp.pprint(metrics)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "    return metrics_list\n",
    "\n",
    "\n",
    "HAUSSDORF = \"haussdorf\"\n",
    "DICE = \"dice\"\n",
    "SENS = \"sens\"\n",
    "SPEC = \"spec\"\n",
    "METRICS = [HAUSSDORF, DICE, SENS, SPEC]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0e2443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                metadata_df,\n",
    "                root_dir,\n",
    "                transform=None,\n",
    "                seg_transform=None, ###\n",
    "                dataformat=None, # indicates what shape (or content) should be returned (2D or 3D, etc.)\n",
    "                returndims=None, # what size/shape 3D volumes should be returned as.\n",
    "                visualize=False,\n",
    "                modality=None,\n",
    "                pad=2,\n",
    "                device='cpu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            metadata_df (string): Path to the csv file w/ patient IDs\n",
    "            root_dir (string): Directory for MR images\n",
    "            transform (callable, optional)\n",
    "        \"\"\"\n",
    "        self.device=device\n",
    "        self.metadata_df = metadata_df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.seg_transform = seg_transform\n",
    "        self.returndims=returndims\n",
    "        self.modality = modality\n",
    "        self.pad = pad\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        print(type(idx), idx)\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        BraTS20ID = self.metadata_df.iloc[idx].BraTS_2020_subject_ID\n",
    "\n",
    "        # make dictonary of paths to MRI volumnes (modalities) and segmenation masks\n",
    "        mr_path_dict = {}\n",
    "        sequence_type = ['seg', 't1', 't1ce', 'flair', 't2']\n",
    "        for seq in sequence_type:\n",
    "            mr_path_dict[seq] = os.path.join(self.root_dir, BraTS20ID, BraTS20ID + '_'+seq+'.nii.gz')\n",
    "\n",
    "        image, seg_image = read_mri(mr_path_dict=mr_path_dict, pad=self.pad)\n",
    "        \n",
    "        if seg_image is not None:\n",
    "            seg_image[seg_image == 4] = 3\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.seg_transform:\n",
    "            seg_image = self.seg_transform(seg_image)\n",
    "        else:\n",
    "            print('no transform')\n",
    "        print(image.shape)\n",
    "        return (image, seg_image), BraTS20ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ae38eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproduciablity\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f880651c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif __name__ == \"__main__\":\\n    import numpy as np\\n    yt = np.random.random(size=(2, 1, 3, 3, 3))\\n    # print(yt)\\n    yt = torch.from_numpy(yt)\\n    yp = np.zeros(shape=(2, 1, 3, 3, 3))\\n    yp = yp + 1\\n    yp = torch.from_numpy(yp)\\n    # print(yp)\\n    dl = DiceLoss()\\n    print(dl(yp, yt).item())\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-5):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        # smooth factor\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, targets, logits):\n",
    "        batch_size = targets.size(0)\n",
    "        # log_prob = torch.sigmoid(logits)\n",
    "        logits = logits.view(batch_size, -1).type(torch.FloatTensor)\n",
    "        targets = targets.view(batch_size, -1).type(torch.FloatTensor)\n",
    "        intersection = (logits * targets).sum(-1)\n",
    "        dice_score = 2. * intersection / ((logits + targets).sum(-1) + self.epsilon)\n",
    "        # dice_score = 1 - dice_score.sum() / batch_size\n",
    "        return torch.mean(1. - dice_score)\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    import numpy as np\n",
    "    yt = np.random.random(size=(2, 1, 3, 3, 3))\n",
    "    # print(yt)\n",
    "    yt = torch.from_numpy(yt)\n",
    "    yp = np.zeros(shape=(2, 1, 3, 3, 3))\n",
    "    yp = yp + 1\n",
    "    yp = torch.from_numpy(yp)\n",
    "    # print(yp)\n",
    "    dl = DiceLoss()\n",
    "    print(dl(yp, yt).item())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd4917d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of input:  torch.Size([4, 4, 144, 144, 144])\n",
      "conv_0_0 torch.Size([4, 8, 144, 144, 144])\n",
      "conv_0_1 torch.Size([4, 16, 144, 144, 144])\n",
      "max_pooling_0 torch.Size([4, 16, 72, 72, 72])\n",
      "conv_1_0 torch.Size([4, 16, 72, 72, 72])\n",
      "conv_1_1 torch.Size([4, 32, 72, 72, 72])\n",
      "max_pooling_1 torch.Size([4, 32, 36, 36, 36])\n",
      "conv_2_0 torch.Size([4, 32, 36, 36, 36])\n",
      "conv_2_1 torch.Size([4, 64, 36, 36, 36])\n",
      "max_pooling_2 torch.Size([4, 64, 18, 18, 18])\n",
      "conv_3_0 torch.Size([4, 64, 18, 18, 18])\n",
      "conv_3_1 torch.Size([4, 128, 18, 18, 18])\n",
      "deconv_2 torch.Size([4, 128, 36, 36, 36])\n",
      "conv_2_0 torch.Size([4, 64, 36, 36, 36])\n",
      "conv_2_1 torch.Size([4, 64, 36, 36, 36])\n",
      "deconv_1 torch.Size([4, 64, 72, 72, 72])\n",
      "conv_1_0 torch.Size([4, 32, 72, 72, 72])\n",
      "conv_1_1 torch.Size([4, 32, 72, 72, 72])\n",
      "deconv_0 torch.Size([4, 32, 144, 144, 144])\n",
      "conv_0_0 torch.Size([4, 16, 144, 144, 144])\n",
      "conv_0_1 torch.Size([4, 16, 144, 144, 144])\n",
      "final_conv torch.Size([4, 4, 144, 144, 144])\n"
     ]
    }
   ],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, k_size=3, stride=1, padding=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv3d = nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=k_size,\n",
    "                                stride=stride, padding=padding)\n",
    "        self.batch_norm = nn.BatchNorm3d(num_features=out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm(self.conv3d(x))\n",
    "        # x = self.conv3d(x)\n",
    "        x = F.elu(x) #!\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvTranspose(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, k_size=3, stride=2, padding=1, output_padding=1):\n",
    "        super(ConvTranspose, self).__init__()\n",
    "        self.conv3d_transpose = nn.ConvTranspose3d(in_channels=in_channels,\n",
    "                                                   out_channels=out_channels,\n",
    "                                                   kernel_size=k_size,\n",
    "                                                   stride=stride,\n",
    "                                                   padding=padding,\n",
    "                                                   output_padding=output_padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv3d_transpose(x)\n",
    "\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, init_features, model_depth=4, pool_size=2):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.root_feat_maps = init_features\n",
    "        self.num_conv_blocks = 2\n",
    "        # self.module_list = nn.ModuleList()\n",
    "        self.module_dict = nn.ModuleDict()\n",
    "        for depth in range(model_depth):\n",
    "            feat_map_channels = 2 ** (depth + 1) * self.root_feat_maps\n",
    "            for i in range(self.num_conv_blocks):\n",
    "                # print(\"depth {}, conv {}\".format(depth, i))\n",
    "                if depth == 0:\n",
    "                    # print(in_channels, feat_map_channels)\n",
    "                    self.conv_block = ConvBlock(in_channels=in_channels, out_channels=feat_map_channels)\n",
    "                    self.module_dict[\"conv_{}_{}\".format(depth, i)] = self.conv_block\n",
    "                    in_channels, feat_map_channels = feat_map_channels, feat_map_channels * 2\n",
    "                else:\n",
    "                    # print(in_channels, feat_map_channels)\n",
    "                    self.conv_block = ConvBlock(in_channels=in_channels, out_channels=feat_map_channels)\n",
    "                    self.module_dict[\"conv_{}_{}\".format(depth, i)] = self.conv_block\n",
    "                    in_channels, feat_map_channels = feat_map_channels, feat_map_channels * 2\n",
    "            if depth == model_depth - 1:\n",
    "                break\n",
    "            else:\n",
    "                self.pooling = nn.MaxPool3d(kernel_size=pool_size, stride=2, padding=0)\n",
    "                self.module_dict[\"max_pooling_{}\".format(depth)] = self.pooling\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_sampling_features = []\n",
    "        for k, op in self.module_dict.items():\n",
    "            if k.startswith(\"conv\"):\n",
    "                x = op(x)\n",
    "                print(k, x.shape)\n",
    "                if k.endswith(\"1\"):\n",
    "                    down_sampling_features.append(x)\n",
    "            elif k.startswith(\"max_pooling\"):\n",
    "                x = op(x)\n",
    "                print(k, x.shape)\n",
    "\n",
    "        return x, down_sampling_features\n",
    "\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, out_channels, init_features, model_depth=4):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.num_conv_blocks = 2\n",
    "        self.num_feat_maps = init_features\n",
    "        # user nn.ModuleDict() to store ops\n",
    "        self.module_dict = nn.ModuleDict()\n",
    "\n",
    "        for depth in range(model_depth - 2, -1, -1):\n",
    "            # print(depth)\n",
    "            feat_map_channels = 2 ** (depth + 1) * self.num_feat_maps\n",
    "            # print(feat_map_channels * 4)\n",
    "            self.deconv = ConvTranspose(in_channels=feat_map_channels * 4, out_channels=feat_map_channels * 4)\n",
    "            self.module_dict[\"deconv_{}\".format(depth)] = self.deconv\n",
    "            for i in range(self.num_conv_blocks):\n",
    "                if i == 0:\n",
    "                    self.conv = ConvBlock(in_channels=feat_map_channels * 6, out_channels=feat_map_channels * 2)\n",
    "                    self.module_dict[\"conv_{}_{}\".format(depth, i)] = self.conv\n",
    "                else:\n",
    "                    self.conv = ConvBlock(in_channels=feat_map_channels * 2, out_channels=feat_map_channels * 2)\n",
    "                    self.module_dict[\"conv_{}_{}\".format(depth, i)] = self.conv\n",
    "            if depth == 0:\n",
    "                self.final_conv = ConvBlock(in_channels=feat_map_channels * 2, out_channels=out_channels)\n",
    "                self.module_dict[\"final_conv\"] = self.final_conv\n",
    "\n",
    "    def forward(self, x, down_sampling_features):\n",
    "        \"\"\"\n",
    "        :param x: inputs\n",
    "        :param down_sampling_features: feature maps from encoder path\n",
    "        :return: output\n",
    "        \"\"\"\n",
    "        for k, op in self.module_dict.items():\n",
    "            if k.startswith(\"deconv\"):\n",
    "                x = op(x)\n",
    "                print(k, x.shape)\n",
    "                x = torch.cat((down_sampling_features[int(k[-1])], x), dim=1)\n",
    "            elif k.startswith(\"conv\"):\n",
    "                x = op(x)\n",
    "                print(k, x.shape)\n",
    "            else:\n",
    "                x = op(x)\n",
    "                print(k, x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # x has shape of (batch_size, channels, depth, height, width)\n",
    "    x_test = torch.randn(4, 4, 144, 144, 144)\n",
    "    #x_test = x_test.cuda()\n",
    "    print(\"The shape of input: \", x_test.shape)\n",
    "    in_channels = out_channels = 4\n",
    "    init_features = 4\n",
    "    encoder = EncoderBlock(in_channels, init_features)\n",
    "    #encoder.cuda()\n",
    "    #print(encoder)\n",
    "    x_test, h = encoder(x_test)\n",
    "\n",
    "    db = DecoderBlock(out_channels, init_features)\n",
    "    #db.cuda()\n",
    "    #print(db)\n",
    "    x_test = db(x_test, h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fca5e88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet3D(\n",
      "  (encoders): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (basic_module): DoubleConv(\n",
      "        (SingleConv1): SingleConv(\n",
      "          (conv): Conv3d(3, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (batchnorm): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (ELU): ELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (SingleConv2): SingleConv(\n",
      "          (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (batchnorm): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (ELU): ELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (pooling): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (basic_module): DoubleConv(\n",
      "        (SingleConv1): SingleConv(\n",
      "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (batchnorm): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (ELU): ELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (SingleConv2): SingleConv(\n",
      "          (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (batchnorm): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (ELU): ELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Encoder(\n",
      "      (pooling): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (basic_module): DoubleConv(\n",
      "        (SingleConv1): SingleConv(\n",
      "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (batchnorm): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (ELU): ELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (SingleConv2): SingleConv(\n",
      "          (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (batchnorm): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (ELU): ELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Encoder(\n",
      "      (pooling): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (basic_module): DoubleConv(\n",
      "        (SingleConv1): SingleConv(\n",
      "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (batchnorm): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (ELU): ELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (SingleConv2): SingleConv(\n",
      "          (conv): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (batchnorm): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (ELU): ELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoders): ModuleList(\n",
      "    (0): Decoder(\n",
      "      (upsampling): TransposeConvUpsampling(\n",
      "        (upsample): ConvTranspose3d(768, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "      )\n",
      "      (basic_module): DoubleConv(\n",
      "        (SingleConv1): SingleConv(\n",
      "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (batchnorm): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (ELU): ELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (SingleConv2): SingleConv(\n",
      "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (batchnorm): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (ELU): ELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Decoder(\n",
      "      (upsampling): TransposeConvUpsampling(\n",
      "        (upsample): ConvTranspose3d(384, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "      )\n",
      "      (basic_module): DoubleConv(\n",
      "        (SingleConv1): SingleConv(\n",
      "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (batchnorm): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (ELU): ELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (SingleConv2): SingleConv(\n",
      "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (batchnorm): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (ELU): ELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Decoder(\n",
      "      (upsampling): TransposeConvUpsampling(\n",
      "        (upsample): ConvTranspose3d(192, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "      )\n",
      "      (basic_module): DoubleConv(\n",
      "        (SingleConv1): SingleConv(\n",
      "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (batchnorm): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (ELU): ELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (SingleConv2): SingleConv(\n",
      "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (batchnorm): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (ELU): ELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_conv): Conv3d(64, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "  (final_activation): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def number_of_features_per_level(init_channel_number, num_levels):\n",
    "    return [init_channel_number * 2 ** k for k in range(num_levels)]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # x has shape of (batch_size, channels, depth, height, width)\n",
    "    model3 = UNet3D(in_channels=3, out_channels=3,\n",
    "                      f_maps=64, layer_order='cbe')\n",
    "    print(model3)\n",
    "    #x_test = torch.randn(4, 4, 96, 96, 96)\n",
    "    \n",
    "    #x_test = model3(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e82b6ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (encoder1): Sequential(\n",
      "    (enc1conv1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc1norm1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc1relu1): ReLU(inplace=True)\n",
      "    (enc1conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc1norm2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc1relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (encoder2): Sequential(\n",
      "    (enc2conv1): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc2norm1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc2relu1): ReLU(inplace=True)\n",
      "    (enc2conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc2norm2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc2relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (encoder3): Sequential(\n",
      "    (enc3conv1): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc3norm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc3relu1): ReLU(inplace=True)\n",
      "    (enc3conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc3norm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc3relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (encoder4): Sequential(\n",
      "    (enc4conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc4norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc4relu1): ReLU(inplace=True)\n",
      "    (enc4conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc4norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc4relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bottleneck): Sequential(\n",
      "    (bottleneckconv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bottlenecknorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bottleneckrelu1): ReLU(inplace=True)\n",
      "    (bottleneckconv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bottlenecknorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bottleneckrelu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv4): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder4): Sequential(\n",
      "    (dec4conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec4norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec4relu1): ReLU(inplace=True)\n",
      "    (dec4conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec4norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec4relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv3): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder3): Sequential(\n",
      "    (dec3conv1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec3norm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec3relu1): ReLU(inplace=True)\n",
      "    (dec3conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec3norm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec3relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv2): ConvTranspose2d(16, 8, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder2): Sequential(\n",
      "    (dec2conv1): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec2norm1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec2relu1): ReLU(inplace=True)\n",
      "    (dec2conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec2norm2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec2relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv1): ConvTranspose2d(8, 4, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder1): Sequential(\n",
      "    (dec1conv1): Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec1norm1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec1relu1): ReLU(inplace=True)\n",
      "    (dec1conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec1norm2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec1relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "                        in_channels=4, out_channels=4, init_features=4,\n",
    "                        pretrained=False)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ab88c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def conv3d(in_channels, out_channels, kernel_size, bias, padding):\n",
    "    return nn.Conv3d(in_channels, out_channels, kernel_size, padding=padding, bias=bias)\n",
    "\n",
    "\n",
    "def create_conv(in_channels, out_channels, kernel_size, order, num_groups, padding):\n",
    "    \"\"\"\n",
    "    Create a list of modules with together constitute a single conv layer with non-linearity\n",
    "    and optional batchnorm/groupnorm.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        kernel_size(int or tuple): size of the convolving kernel\n",
    "        order (string): order of things, e.g.\n",
    "            'cr' -> conv + ReLU\n",
    "            'gcr' -> groupnorm + conv + ReLU\n",
    "            'cl' -> conv + LeakyReLU\n",
    "            'ce' -> conv + ELU\n",
    "            'bcr' -> batchnorm + conv + ReLU\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
    "\n",
    "    Return:\n",
    "        list of tuple (name, module)\n",
    "    \"\"\"\n",
    "    assert 'c' in order, \"Conv layer MUST be present\"\n",
    "    assert order[0] not in 'rle', 'Non-linearity cannot be the first operation in the layer'\n",
    "\n",
    "    modules = []\n",
    "    for i, char in enumerate(order):\n",
    "        if char == 'r':\n",
    "            modules.append(('ReLU', nn.ReLU(inplace=True)))\n",
    "        elif char == 'l':\n",
    "            modules.append(('LeakyReLU', nn.LeakyReLU(inplace=True)))\n",
    "        elif char == 'e':\n",
    "            modules.append(('ELU', nn.ELU(inplace=True)))\n",
    "        elif char == 'c':\n",
    "            # add learnable bias only in the absence of batchnorm/groupnorm\n",
    "            bias = not ('g' in order or 'b' in order)\n",
    "            modules.append(('conv', conv3d(in_channels, out_channels, kernel_size, bias, padding=padding)))\n",
    "        elif char == 'g':\n",
    "            is_before_conv = i < order.index('c')\n",
    "            if is_before_conv:\n",
    "                num_channels = in_channels\n",
    "            else:\n",
    "                num_channels = out_channels\n",
    "\n",
    "            # use only one group if the given number of groups is greater than the number of channels\n",
    "            if num_channels < num_groups:\n",
    "                num_groups = 1\n",
    "\n",
    "            assert num_channels % num_groups == 0, f'Expected number of channels in input to be divisible by num_groups. num_channels={num_channels}, num_groups={num_groups}'\n",
    "            modules.append(('groupnorm', nn.GroupNorm(num_groups=num_groups, num_channels=num_channels)))\n",
    "        elif char == 'b':\n",
    "            is_before_conv = i < order.index('c')\n",
    "            if is_before_conv:\n",
    "                modules.append(('batchnorm', nn.BatchNorm3d(in_channels)))\n",
    "            else:\n",
    "                modules.append(('batchnorm', nn.BatchNorm3d(out_channels)))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported layer type '{char}'. MUST be one of ['b', 'g', 'r', 'l', 'e', 'c']\")\n",
    "\n",
    "    return modules\n",
    "\n",
    "\n",
    "class SingleConv(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Basic convolutional module consisting of a Conv3d, non-linearity and optional batchnorm/groupnorm. The order\n",
    "    of operations can be specified via the `order` parameter\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        kernel_size (int or tuple): size of the convolving kernel\n",
    "        order (string): determines the order of layers, e.g.\n",
    "            'cr' -> conv + ReLU\n",
    "            'crg' -> conv + ReLU + groupnorm\n",
    "            'cl' -> conv + LeakyReLU\n",
    "            'ce' -> conv + ELU\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        padding (int or tuple):\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, order='cbe', num_groups=8, padding=1):\n",
    "        super(SingleConv, self).__init__()\n",
    "\n",
    "        for name, module in create_conv(in_channels, out_channels, kernel_size, order, num_groups, padding=padding):\n",
    "            self.add_module(name, module)\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Sequential):\n",
    "    \"\"\"\n",
    "    A module consisting of two consecutive convolution layers (e.g. BatchNorm3d+ReLU+Conv3d).\n",
    "    We use (Conv3d+ReLU+GroupNorm3d) by default.\n",
    "    This can be changed however by providing the 'order' argument, e.g. in order\n",
    "    to change to Conv3d+BatchNorm3d+ELU use order='cbe'.\n",
    "    Use padded convolutions to make sure that the output (H_out, W_out) is the same\n",
    "    as (H_in, W_in), so that you don't have to crop in the decoder path.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        encoder (bool): if True we're in the encoder path, otherwise we're in the decoder\n",
    "        kernel_size (int or tuple): size of the convolving kernel\n",
    "        order (string): determines the order of layers, e.g.\n",
    "            'cr' -> conv + ReLU\n",
    "            'crg' -> conv + ReLU + groupnorm\n",
    "            'cl' -> conv + LeakyReLU\n",
    "            'ce' -> conv + ELU\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, encoder, kernel_size=3, order='cbe', num_groups=8, padding=1):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        if encoder:\n",
    "            # we're in the encoder path\n",
    "            conv1_in_channels = in_channels\n",
    "            conv1_out_channels = out_channels // 2\n",
    "            if conv1_out_channels < in_channels:\n",
    "                conv1_out_channels = in_channels\n",
    "            conv2_in_channels, conv2_out_channels = conv1_out_channels, out_channels\n",
    "        else:\n",
    "            # we're in the decoder path, decrease the number of channels in the 1st convolution\n",
    "            conv1_in_channels, conv1_out_channels = in_channels, out_channels\n",
    "            conv2_in_channels, conv2_out_channels = out_channels, out_channels\n",
    "\n",
    "        # conv1\n",
    "        self.add_module('SingleConv1',\n",
    "                        SingleConv(conv1_in_channels, conv1_out_channels, kernel_size, order, num_groups,\n",
    "                                   padding=padding))\n",
    "        # conv2\n",
    "        self.add_module('SingleConv2',\n",
    "                        SingleConv(conv2_in_channels, conv2_out_channels, kernel_size, order, num_groups,\n",
    "                                   padding=padding))\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A single module from the encoder path consisting of the optional max\n",
    "    pooling layer (one may specify the MaxPool kernel_size to be different\n",
    "    than the standard (2,2,2), e.g. if the volumetric data is anisotropic\n",
    "    (make sure to use complementary scale_factor in the decoder path) followed by\n",
    "    a DoubleConv module.\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        conv_kernel_size (int or tuple): size of the convolving kernel\n",
    "        apply_pooling (bool): if True use MaxPool3d before DoubleConv\n",
    "        pool_kernel_size (int or tuple): the size of the window\n",
    "        pool_type (str): pooling layer: 'max' or 'avg'\n",
    "        basic_module(nn.Module): either ResNetBlock or DoubleConv\n",
    "        conv_layer_order (string): determines the order of layers\n",
    "            in `DoubleConv` module. See `DoubleConv` for more info.\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, conv_kernel_size=3, apply_pooling=True,\n",
    "                 pool_kernel_size=2, pool_type='max', basic_module=DoubleConv, conv_layer_order='cbe',\n",
    "                 num_groups=8, padding=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        assert pool_type in ['max', 'avg']\n",
    "        if apply_pooling:\n",
    "            if pool_type == 'max':\n",
    "                self.pooling = nn.MaxPool3d(kernel_size=pool_kernel_size)\n",
    "            else:\n",
    "                self.pooling = nn.AvgPool3d(kernel_size=pool_kernel_size)\n",
    "        else:\n",
    "            self.pooling = None\n",
    "\n",
    "        self.basic_module = basic_module(in_channels, out_channels,\n",
    "                                         encoder=True,\n",
    "                                         kernel_size=conv_kernel_size,\n",
    "                                         order=conv_layer_order,\n",
    "                                         num_groups=num_groups,\n",
    "                                         padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.pooling is not None:\n",
    "            x = self.pooling(x)\n",
    "        x = self.basic_module(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A single module for decoder path consisting of the upsampling layer\n",
    "    (either learned ConvTranspose3d or nearest neighbor interpolation) followed by a basic module (DoubleConv or ExtResNetBlock).\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        conv_kernel_size (int or tuple): size of the convolving kernel\n",
    "        scale_factor (tuple): used as the multiplier for the image H/W/D in\n",
    "            case of nn.Upsample or as stride in case of ConvTranspose3d, must reverse the MaxPool3d operation\n",
    "            from the corresponding encoder\n",
    "        basic_module(nn.Module): either ResNetBlock or DoubleConv\n",
    "        conv_layer_order (string): determines the order of layers\n",
    "            in `DoubleConv` module. See `DoubleConv` for more info.\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
    "        upsample (boole): should the input be upsampled\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, conv_kernel_size=3, scale_factor=(2, 2, 2), basic_module=DoubleConv,\n",
    "                 conv_layer_order='cbe', num_groups=8, mode='nearest', padding=1, upsample=True):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        if upsample:\n",
    "            if basic_module == DoubleConv:\n",
    "                # if DoubleConv is the basic_module use interpolation for upsampling and concatenation joining\n",
    "                #self.upsampling = InterpolateUpsampling(mode=mode)\n",
    "                # concat joining\n",
    "                #self.joining = partial(self._joining, concat=True)\n",
    "                # if basic_module=ExtResNetBlock use transposed convolution upsampling and summation joining\n",
    "                self.upsampling = TransposeConvUpsampling(in_channels=in_channels, out_channels=out_channels,\n",
    "                                                          kernel_size=conv_kernel_size, scale_factor=scale_factor)\n",
    "                # sum joining\n",
    "                self.joining = partial(self._joining, concat=False)\n",
    "                # adapt the number of in_channels for the ExtResNetBlock\n",
    "                in_channels = out_channels\n",
    "            else:\n",
    "                # if basic_module=ExtResNetBlock use transposed convolution upsampling and summation joining\n",
    "                self.upsampling = TransposeConvUpsampling(in_channels=in_channels, out_channels=out_channels,\n",
    "                                                          kernel_size=conv_kernel_size, scale_factor=scale_factor)\n",
    "                # sum joining\n",
    "                self.joining = partial(self._joining, concat=False)\n",
    "                # adapt the number of in_channels for the ExtResNetBlock\n",
    "                in_channels = out_channels\n",
    "        else:\n",
    "            # no upsampling\n",
    "            self.upsampling = NoUpsampling()\n",
    "            # concat joining\n",
    "            self.joining = partial(self._joining, concat=True)\n",
    "\n",
    "        self.basic_module = basic_module(in_channels, out_channels,\n",
    "                                         encoder=False,\n",
    "                                         kernel_size=conv_kernel_size,\n",
    "                                         order=conv_layer_order,\n",
    "                                         num_groups=num_groups,\n",
    "                                         padding=padding)\n",
    "\n",
    "    def forward(self, encoder_features, x):\n",
    "        x = self.upsampling(encoder_features=encoder_features, x=x)\n",
    "        x = self.joining(encoder_features, x)\n",
    "        x = self.basic_module(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def _joining(encoder_features, x, concat):\n",
    "        if concat:\n",
    "            return torch.cat((encoder_features, x), dim=1)\n",
    "        else:\n",
    "            return encoder_features + x\n",
    "\n",
    "\n",
    "def create_encoders(in_channels, f_maps, basic_module, conv_kernel_size, conv_padding, layer_order, num_groups,\n",
    "                    pool_kernel_size):\n",
    "    # create encoder path consisting of Encoder modules. Depth of the encoder is equal to `len(f_maps)`\n",
    "    encoders = []\n",
    "    for i, out_feature_num in enumerate(f_maps):\n",
    "        if i == 0:\n",
    "            encoder = Encoder(in_channels, out_feature_num,\n",
    "                              apply_pooling=False,  # skip pooling in the firs encoder\n",
    "                              basic_module=basic_module,\n",
    "                              conv_layer_order=layer_order,\n",
    "                              conv_kernel_size=conv_kernel_size,\n",
    "                              num_groups=num_groups,\n",
    "                              padding=conv_padding)\n",
    "        else:\n",
    "            # TODO: adapt for anisotropy in the data, i.e. use proper pooling kernel to make the data isotropic after 1-2 pooling operations\n",
    "            encoder = Encoder(f_maps[i - 1], out_feature_num,\n",
    "                              basic_module=basic_module,\n",
    "                              conv_layer_order=layer_order,\n",
    "                              conv_kernel_size=conv_kernel_size,\n",
    "                              num_groups=num_groups,\n",
    "                              pool_kernel_size=pool_kernel_size,\n",
    "                              padding=conv_padding)\n",
    "\n",
    "        encoders.append(encoder)\n",
    "\n",
    "    return nn.ModuleList(encoders)\n",
    "\n",
    "\n",
    "def create_decoders(f_maps, basic_module, conv_kernel_size, conv_padding, layer_order, num_groups, upsample):\n",
    "    # create decoder path consisting of the Decoder modules. The length of the decoder list is equal to `len(f_maps) - 1`\n",
    "    decoders = []\n",
    "    reversed_f_maps = list(reversed(f_maps))\n",
    "    for i in range(len(reversed_f_maps) - 1):\n",
    "        if basic_module == DoubleConv:\n",
    "            in_feature_num = reversed_f_maps[i] + reversed_f_maps[i + 1]\n",
    "        else:\n",
    "            in_feature_num = reversed_f_maps[i]\n",
    "\n",
    "        out_feature_num = reversed_f_maps[i + 1]\n",
    "\n",
    "        # TODO: if non-standard pooling was used, make sure to use correct striding for transpose conv\n",
    "        # currently strides with a constant stride: (2, 2, 2)\n",
    "\n",
    "        _upsample = True\n",
    "        if i == 0:\n",
    "            # upsampling can be skipped only for the 1st decoder, afterwards it should always be present\n",
    "            _upsample = upsample\n",
    "\n",
    "        decoder = Decoder(in_feature_num, out_feature_num,\n",
    "                          basic_module=basic_module,\n",
    "                          conv_layer_order=layer_order,\n",
    "                          conv_kernel_size=conv_kernel_size,\n",
    "                          num_groups=num_groups,\n",
    "                          padding=conv_padding,\n",
    "                          upsample=_upsample)\n",
    "        decoders.append(decoder)\n",
    "    return nn.ModuleList(decoders)\n",
    "\n",
    "\n",
    "class AbstractUpsampling(nn.Module):\n",
    "    \"\"\"\n",
    "    Abstract class for upsampling. A given implementation should upsample a given 5D input tensor using either\n",
    "    interpolation or learned transposed convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, upsample):\n",
    "        super(AbstractUpsampling, self).__init__()\n",
    "        self.upsample = upsample\n",
    "\n",
    "    def forward(self, encoder_features, x):\n",
    "        # get the spatial dimensions of the output given the encoder_features\n",
    "        output_size = encoder_features.size()[2:]\n",
    "        # upsample the input and return\n",
    "        return self.upsample(x, output_size)\n",
    "\n",
    "\n",
    "class InterpolateUpsampling(AbstractUpsampling):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        mode (str): algorithm used for upsampling:\n",
    "            'nearest' | 'linear' | 'bilinear' | 'trilinear' | 'area'. Default: 'nearest'\n",
    "            used only if transposed_conv is False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mode='nearest'):\n",
    "        upsample = partial(self._interpolate, mode=mode)\n",
    "        super().__init__(upsample)\n",
    "\n",
    "    @staticmethod\n",
    "    def _interpolate(x, size, mode):\n",
    "        return F.interpolate(x, size=size, mode=mode)\n",
    "\n",
    "\n",
    "class TransposeConvUpsampling(AbstractUpsampling):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        in_channels (int): number of input channels for transposed conv\n",
    "            used only if transposed_conv is True\n",
    "        out_channels (int): number of output channels for transpose conv\n",
    "            used only if transposed_conv is True\n",
    "        kernel_size (int or tuple): size of the convolving kernel\n",
    "            used only if transposed_conv is True\n",
    "        scale_factor (int or tuple): stride of the convolution\n",
    "            used only if transposed_conv is True\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=None, out_channels=None, kernel_size=3, scale_factor=(2, 2, 2)):\n",
    "        # make sure that the output size reverses the MaxPool3d from the corresponding encoder\n",
    "        upsample = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=kernel_size, stride=scale_factor,\n",
    "                                      padding=1)\n",
    "        super().__init__(upsample)\n",
    "\n",
    "\n",
    "class NoUpsampling(AbstractUpsampling):\n",
    "    def __init__(self):\n",
    "        super().__init__(self._no_upsampling)\n",
    "\n",
    "    @staticmethod\n",
    "    def _no_upsampling(x, size):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d033c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Abstract3DUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class for standard and residual UNet.\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output segmentation masks;\n",
    "            Note that that the of out_channels might correspond to either\n",
    "            different semantic classes or to different binary segmentation mask.\n",
    "            It's up to the user of the class to interpret the out_channels and\n",
    "            use the proper loss criterion during training (i.e. CrossEntropyLoss (multi-class)\n",
    "            or BCEWithLogitsLoss (two-class) respectively)\n",
    "        f_maps (int, tuple): number of feature maps at each level of the encoder; if it's an integer the number\n",
    "            of feature maps is given by the geometric progression: f_maps ^ k, k=1,2,3,4\n",
    "        final_sigmoid (bool): if True apply element-wise nn.Sigmoid after the\n",
    "            final 1x1 convolution, otherwise apply nn.Softmax. MUST be True if nn.BCELoss (two-class) is used\n",
    "            to train the model. MUST be False if nn.CrossEntropyLoss (multi-class) is used to train the model.\n",
    "        basic_module: basic model for the encoder/decoder (DoubleConv, ExtResNetBlock, ....)\n",
    "        layer_order (string): determines the order of layers\n",
    "            in `SingleConv` module. e.g. 'crg' stands for Conv3d+ReLU+GroupNorm3d.\n",
    "            See `SingleConv` for more info\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        num_levels (int): number of levels in the encoder/decoder path (applied only if f_maps is an int)\n",
    "        is_segmentation (bool): if True (semantic segmentation problem) Sigmoid/Softmax normalization is applied\n",
    "            after the final convolution; if False (regression problem) the normalization layer is skipped at the end\n",
    "        testing (bool): if True (testing mode) the `final_activation` (if present, i.e. `is_segmentation=true`)\n",
    "            will be applied as the last operation during the forward pass; if False the model is in training mode\n",
    "            and the `final_activation` (even if present) won't be applied; default: False\n",
    "        conv_kernel_size (int or tuple): size of the convolving kernel in the basic_module\n",
    "        pool_kernel_size (int or tuple): the size of the window\n",
    "        conv_padding (int or tuple): add zero-padding added to all three sides of the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, final_sigmoid, basic_module, f_maps=64, layer_order='gcr',\n",
    "                 num_groups=8, num_levels=4, is_segmentation=True, testing=False,\n",
    "                 conv_kernel_size=3, pool_kernel_size=2, conv_padding=1, **kwargs):\n",
    "        super(Abstract3DUNet, self).__init__()\n",
    "\n",
    "        self.testing = testing\n",
    "\n",
    "        if isinstance(f_maps, int):\n",
    "            f_maps = number_of_features_per_level(f_maps, num_levels=num_levels)\n",
    "\n",
    "        assert isinstance(f_maps, list) or isinstance(f_maps, tuple)\n",
    "        assert len(f_maps) > 1, \"Required at least 2 levels in the U-Net\"\n",
    "\n",
    "        # create encoder path\n",
    "        self.encoders = create_encoders(in_channels, f_maps, basic_module, conv_kernel_size, conv_padding, layer_order,\n",
    "                                        num_groups, pool_kernel_size)\n",
    "\n",
    "        # create decoder path\n",
    "        self.decoders = create_decoders(f_maps, basic_module, conv_kernel_size, conv_padding, layer_order, num_groups,\n",
    "                                        upsample=True)\n",
    "\n",
    "        # in the last layer a 11 convolution reduces the number of output\n",
    "        # channels to the number of labels\n",
    "        self.final_conv = nn.Conv3d(f_maps[0], out_channels, 1)\n",
    "\n",
    "        if is_segmentation:\n",
    "            # semantic segmentation problem\n",
    "            if final_sigmoid:\n",
    "                self.final_activation = nn.Sigmoid()\n",
    "            else:\n",
    "                self.final_activation = nn.Softmax(dim=1)\n",
    "        else:\n",
    "            # regression problem\n",
    "            self.final_activation = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder part\n",
    "        encoders_features = []\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "            # reverse the encoder outputs to be aligned with the decoder\n",
    "            encoders_features.insert(0, x)\n",
    "\n",
    "        # remove the last encoder's output from the list\n",
    "        # !!remember: it's the 1st in the list\n",
    "        encoders_features = encoders_features[1:]\n",
    "\n",
    "        # decoder part\n",
    "        for decoder, encoder_features in zip(self.decoders, encoders_features):\n",
    "            # pass the output from the corresponding encoder and the output\n",
    "            # of the previous decoder\n",
    "            x = decoder(encoder_features, x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # apply final_activation (i.e. Sigmoid or Softmax) only during prediction. During training the network outputs\n",
    "        # logits and it's up to the user to normalize it before visualising with tensorboard or computing validation metric\n",
    "        if self.testing and self.final_activation is not None:\n",
    "            x = self.final_activation(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet3D(Abstract3DUNet):\n",
    "    \"\"\"\n",
    "    3DUnet model from\n",
    "    `\"3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation\"\n",
    "        <https://arxiv.org/pdf/1606.06650.pdf>`.\n",
    "    Uses `DoubleConv` as a basic_module and nearest neighbor upsampling in the decoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=64, layer_order='gcr',\n",
    "                 num_groups=8, num_levels=4, is_segmentation=True, conv_padding=1, **kwargs):\n",
    "        super(UNet3D, self).__init__(in_channels=in_channels,\n",
    "                                     out_channels=out_channels,\n",
    "                                     final_sigmoid=final_sigmoid,\n",
    "                                     basic_module=DoubleConv,\n",
    "                                     f_maps=f_maps,\n",
    "                                     layer_order=layer_order,\n",
    "                                     num_groups=num_groups,\n",
    "                                     num_levels=num_levels,\n",
    "                                     is_segmentation=is_segmentation,\n",
    "                                     conv_padding=conv_padding,\n",
    "                                     **kwargs)\n",
    "\n",
    "\n",
    "class UNet2D(Abstract3DUNet):\n",
    "    \"\"\"\n",
    "    Just a standard 2D Unet. Arises naturally by specifying conv_kernel_size=(1, 3, 3), pool_kernel_size=(1, 2, 2).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=64, layer_order='gcr',\n",
    "                 num_groups=8, num_levels=4, is_segmentation=True, conv_padding=1, **kwargs):\n",
    "        if conv_padding == 1:\n",
    "            conv_padding = (0, 1, 1)\n",
    "        super(UNet2D, self).__init__(in_channels=in_channels,\n",
    "                                     out_channels=out_channels,\n",
    "                                     final_sigmoid=final_sigmoid,\n",
    "                                     basic_module=DoubleConv,\n",
    "                                     f_maps=f_maps,\n",
    "                                     layer_order=layer_order,\n",
    "                                     num_groups=num_groups,\n",
    "                                     num_levels=num_levels,\n",
    "                                     is_segmentation=is_segmentation,\n",
    "                                     conv_kernel_size=(1, 3, 3),\n",
    "                                     pool_kernel_size=(1, 2, 2),\n",
    "                                     conv_padding=conv_padding,\n",
    "                                     **kwargs)\n",
    "\n",
    "\n",
    "def get_model(model_config):\n",
    "    def _model_class(class_name):\n",
    "        modules = ['pytorch3dunet.unet3d.model']\n",
    "        for module in modules:\n",
    "            m = importlib.import_module(module)\n",
    "            clazz = getattr(m, class_name, None)\n",
    "            if clazz is not None:\n",
    "                return clazz\n",
    "\n",
    "    model_class = _model_class(model_config['name'])\n",
    "    return model_class(**model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19e2bbea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c6833d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bec62b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b1281e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4af1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42042be7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10186787",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, init_features, model_depth=4, final_activation=\"sigmoid\"):\n",
    "        super(UnetModel, self).__init__()\n",
    "        self.encoder = EncoderBlock(in_channels=in_channels,\n",
    "                                    init_features=init_features,\n",
    "                                    model_depth=model_depth)\n",
    "        self.decoder = DecoderBlock(out_channels=out_channels,\n",
    "                                    init_features=init_features,\n",
    "                                    model_depth=model_depth)\n",
    "        if final_activation == \"sigmoid\":\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "        else:\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, downsampling_features = self.encoder(x)\n",
    "        x = self.decoder(x, downsampling_features)\n",
    "        x = self.sigmoid(x)\n",
    "        print(\"Final output shape: \", x.shape)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de5c0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldRun(k_folds, num_epochs, train_batch_size, train_data, validation_data, network, criterion, optim, use_cuda=True):\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    if use_cuda:\n",
    "        network = network.cuda()\n",
    "\n",
    "    loss_function = criterion\n",
    "\n",
    "    dataset = ConcatDataset([train_data, validation_data])\n",
    "\n",
    "    # Define the K-fold Cross Validator\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "    # Start print\n",
    "    print('--------------------------------')\n",
    "\n",
    "    # K-fold Cross Validation model evaluation\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "        # Print\n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "    \n",
    "        # Sample elements randomly from a given list of ids, no replacement.\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "        # Define data loaders for training and testing data in this fold\n",
    "        dataloader_train = DataLoader(dataset, batch_size=train_batch_size,sampler=train_subsampler,num_workers=0)\n",
    "        dataloader_valid = DataLoader(dataset, batch_size=train_batch_size,sampler=test_subsampler, num_workers=0)\n",
    "\n",
    "        # Initialize optimizer\n",
    "        optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
    "\n",
    "        # Run the training loop for defined number of epochs\n",
    "        for epoch in range(0, num_epochs):\n",
    "            # Print epoch\n",
    "            print(f'Starting epoch {epoch+1}')\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Set current loss value\n",
    "            current_loss = 0.0\n",
    "\n",
    "            # Iterate over the DataLoader for training data\n",
    "            if dataloader_train is None or optimizer is None:\n",
    "                print('None')\n",
    "                break  # NotImplementedError\n",
    "            for i, data in enumerate(tqdm.tqdm(dataloader_train)):\n",
    "                # Get inputs\n",
    "                (inputs, targets), ID = data\n",
    "                #inputs = torch.squeeze(torch.permute(image, (0, 4, 1, 2, 3))) \n",
    "                #label = torch.squeeze(torch.permute(seg_image, (0, 4, 1, 2, 3))) \n",
    "                if use_cuda:\n",
    "                    inputs, targets = inputs.cuda(), targets.cuda() # add this line\n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "                # Perform forward pass\n",
    "                outputs = network(inputs)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_function(outputs, targets.squeeze().long())\n",
    "                print('Loss:', loss.item())\n",
    "                # Perform backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Perform optimization\n",
    "                optimizer.step()\n",
    "\n",
    "                # Print statistics\n",
    "                current_loss += loss.item()\n",
    "                if i % 500 == 499:\n",
    "                    print('Loss after mini-batch %5d: %.3f' %\n",
    "                          (i + 1, current_loss / 500))\n",
    "                    current_loss = 0.0\n",
    "            end_time = time.time()\n",
    "            print(f\"Epoch Time: {end_time - start_time}\")\n",
    "    # Process is complete.\n",
    "    print('Training process has finished. Saving trained model.')\n",
    "    \n",
    "    # Saving the model\n",
    "    save_path = f'./model-fold-{fold}.pth'\n",
    "    torch.save(network.state_dict(), 'drive/MyDrive/Colab Notebooks/')\n",
    "\n",
    "    # Print about testing\n",
    "    print('Starting testing')\n",
    "    # Evaluationfor this fold\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the test data and generate predictions\n",
    "        for i, data in enumerate(dataloader_valid, 0):\n",
    "            # Get inputs\n",
    "            inputs, targets = data\n",
    "            # inputs = torch.squeeze(torch.permute(image, (0, 4, 1, 2, 3))) # \n",
    "            # label = torch.squeeze(torch.permute(seg_image, (0, 4, 1, 2, 3))) #, 0) \n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda() # add this line\n",
    "\n",
    "            # Generate outputs\n",
    "            outputs = network(inputs)\n",
    "\n",
    "            # Set total and correct\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "        # Print accuracy\n",
    "        print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n",
    "        print('--------------------------------')\n",
    "        results[fold] = 100.0 * (correct / total)\n",
    "    \n",
    "    # Print fold results\n",
    "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "    print(f'Average: {sum/len(results.items())} %')\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a92e81a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_main(train, valid, in_channels, out_channels, init_features, learning_rate, k_folds, no_epochs, train_batch_size):\n",
    "    \"\"\"\n",
    "    Train module\n",
    "    :param data_folder: data folder\n",
    "    :param in_channels: the input channel of input images\n",
    "    :param out_channels: the final output channel\n",
    "    :param learning_rate: set learning rate for training\n",
    "    :param no_epochs: number of epochs to train model\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    model = UnetModel(in_channels=in_channels, out_channels=out_channels,\n",
    "                      init_features=init_features)\n",
    "    optim = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    #criterion = DiceLoss()\n",
    "    criterion = CrossEntropyLoss()\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    # kFoldRun(k_folds, no_epochs, train_batch_size, train, valid, model, criterion, optim, use_cuda)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef75e79",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc3667e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = '../../MICCAI_BraTS2020_TrainingData/'\n",
    "naming = pd.read_csv(f'../../MICCAI_BraTS2020_TrainingData/name_mapping.csv') # , index_col=0)\n",
    "data_df = pd.DataFrame(naming['BraTS_2020_subject_ID'])\n",
    "n_p = 20 # n_patients_to_train_with\n",
    "train_df = data_df[:n_p]\n",
    "valid_df = data_df[n_p:n_p*2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4280a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset_train = GeneralDataset(metadata_df=train_df, \n",
    "                                           root_dir=image_dir,\n",
    "                                           transform=train_transformations,\n",
    "                                           seg_transform=seg_transformations,\n",
    "                                           returndims=resize_shape)\n",
    "\n",
    "transformed_dataset_valid = GeneralDataset(metadata_df=valid_df, \n",
    "                                           root_dir=image_dir,\n",
    "                                           transform=train_transformations,\n",
    "                                           seg_transform=seg_transformations,\n",
    "                                           returndims=resize_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b092cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 4\n",
    "resize_shape = (144,144,144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c20dd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de0caa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnetModel(\n",
      "  (encoder): EncoderBlock(\n",
      "    (module_dict): ModuleDict(\n",
      "      (conv_0_0): ConvBlock(\n",
      "        (conv3d): Conv3d(4, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (batch_norm): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv_0_1): ConvBlock(\n",
      "        (conv3d): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (batch_norm): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (max_pooling_0): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (conv_1_0): ConvBlock(\n",
      "        (conv3d): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (batch_norm): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv_1_1): ConvBlock(\n",
      "        (conv3d): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (batch_norm): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (max_pooling_1): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (conv_2_0): ConvBlock(\n",
      "        (conv3d): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (batch_norm): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv_2_1): ConvBlock(\n",
      "        (conv3d): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (batch_norm): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (max_pooling_2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (conv_3_0): ConvBlock(\n",
      "        (conv3d): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (batch_norm): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv_3_1): ConvBlock(\n",
      "        (conv3d): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (batch_norm): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (conv_block): ConvBlock(\n",
      "      (conv3d): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (batch_norm): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (pooling): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (decoder): DecoderBlock(\n",
      "    (module_dict): ModuleDict(\n",
      "      (deconv_2): ConvTranspose(\n",
      "        (conv3d_transpose): ConvTranspose3d(512, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "      )\n",
      "      (conv_2_0): ConvBlock(\n",
      "        (conv3d): Conv3d(768, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (batch_norm): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv_2_1): ConvBlock(\n",
      "        (conv3d): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (batch_norm): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (deconv_1): ConvTranspose(\n",
      "        (conv3d_transpose): ConvTranspose3d(256, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "      )\n",
      "      (conv_1_0): ConvBlock(\n",
      "        (conv3d): Conv3d(384, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (batch_norm): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv_1_1): ConvBlock(\n",
      "        (conv3d): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (batch_norm): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (deconv_0): ConvTranspose(\n",
      "        (conv3d_transpose): ConvTranspose3d(128, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "      )\n",
      "      (conv_0_0): ConvBlock(\n",
      "        (conv3d): Conv3d(192, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (batch_norm): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv_0_1): ConvBlock(\n",
      "        (conv3d): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (batch_norm): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (final_conv): ConvBlock(\n",
      "        (conv3d): Conv3d(64, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (batch_norm): BatchNorm3d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (deconv): ConvTranspose(\n",
      "      (conv3d_transpose): ConvTranspose3d(128, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1))\n",
      "    )\n",
      "    (conv): ConvBlock(\n",
      "      (conv3d): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (batch_norm): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (final_conv): ConvBlock(\n",
      "      (conv3d): Conv3d(64, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (batch_norm): BatchNorm3d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "f = train_main(transformed_dataset_train, transformed_dataset_valid, 4, 4, 1e-4, 2, 5, 4)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc1fb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_main(transformed_dataset_train, transformed_dataset_valid, 4, 4, 1e-4, 2, 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535877c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08145b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c466f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc92b9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/wisdomikezogwo/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "                        in_channels=4, out_channels=4, init_features=4,\n",
    "                        pretrained=False)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b97b6a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (encoder1): Sequential(\n",
      "    (enc1conv1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc1norm1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc1relu1): ReLU(inplace=True)\n",
      "    (enc1conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc1norm2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc1relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (encoder2): Sequential(\n",
      "    (enc2conv1): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc2norm1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc2relu1): ReLU(inplace=True)\n",
      "    (enc2conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc2norm2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc2relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (encoder3): Sequential(\n",
      "    (enc3conv1): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc3norm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc3relu1): ReLU(inplace=True)\n",
      "    (enc3conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc3norm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc3relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (encoder4): Sequential(\n",
      "    (enc4conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc4norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc4relu1): ReLU(inplace=True)\n",
      "    (enc4conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc4norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc4relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bottleneck): Sequential(\n",
      "    (bottleneckconv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bottlenecknorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bottleneckrelu1): ReLU(inplace=True)\n",
      "    (bottleneckconv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bottlenecknorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bottleneckrelu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv4): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder4): Sequential(\n",
      "    (dec4conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec4norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec4relu1): ReLU(inplace=True)\n",
      "    (dec4conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec4norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec4relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv3): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder3): Sequential(\n",
      "    (dec3conv1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec3norm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec3relu1): ReLU(inplace=True)\n",
      "    (dec3conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec3norm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec3relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv2): ConvTranspose2d(16, 8, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder2): Sequential(\n",
      "    (dec2conv1): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec2norm1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec2relu1): ReLU(inplace=True)\n",
      "    (dec2conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec2norm2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec2relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv1): ConvTranspose2d(8, 4, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder1): Sequential(\n",
      "    (dec1conv1): Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec1norm1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec1relu1): ReLU(inplace=True)\n",
      "    (dec1conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec1norm2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec1relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training loop here\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    if dataloader_train is None or optimizer is None:\n",
    "        break  # NotImplementedError\n",
    "    for data in tqdm.tqdm(dataloader_train):\n",
    "        (image, seg_image), bratsID = data\n",
    "        \n",
    "        p_image = torch.squeeze(torch.permute(image, (0, 4, 1, 2, 3))) \n",
    "        p_seg_image = torch.squeeze(torch.permute(seg_image, (0, 4, 1, 2, 3))) \n",
    "\n",
    "        if use_cuda:\n",
    "            p_image, p_seg_image = p_image.cuda(), p_seg_image.cuda() \n",
    "        pred = model(p_image.float())\n",
    "        loss = criterion(pred, p_seg_image.long())\n",
    "        \n",
    "        print(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    print(\"Epoch:\", epoch, \"Mean Loss:\", np.mean(losses))\n",
    "\n",
    "###End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc4fcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, 'drive/MyDrive/Colab Notebooks/resumable_third_10_50e_mriseg.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c6c212",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = torch.load('second_100_mriseg.pt', map_location=torch.device('cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1150012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred = model(p_image.float())\n",
    "#pred = model(p_image[:, 0, :, :].unsqueeze(1).float()) # only for 0(flair) modality model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ee5e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice = 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef01764",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.resize(pred[slice, 2, :, :].detach().cpu().numpy(), (160, 160), interpolation = cv2.INTER_NEAREST))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7003f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(pred[0, 1:, 0, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1322db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_seg = torch.sigmoid(pred)\n",
    "segs = pred_seg.permute(1, 0, 2, 3).detach().cpu().numpy() > 0.52   #4, 144, 144, 144 now i.e c, d, h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acbb4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0, :, 0, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fecea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_seg[0, :, 0, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e03305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On predictions- get Regions\n",
    "et = segs[1]\n",
    "net = np.logical_and(segs[2], np.logical_not(et))\n",
    "ed = np.logical_and(segs[3], np.logical_not(segs[2]))\n",
    "\n",
    "labelmap = np.zeros(segs[1].shape)\n",
    "labelmap[et] = 3\n",
    "labelmap[net] = 1\n",
    "labelmap[ed] = 2\n",
    "\n",
    "print(f\"voxel values : {np.unique(labelmap)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7dc6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize any 2d slice\n",
    "\n",
    "plt.imshow(labelmap[slice])\n",
    "print(f\"pixel values: {np.unique(labelmap[slice])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5526d9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d85fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_seg = p_seg_image.squeeze()\n",
    "refmap_et, refmap_tc, refmap_wt = [np.zeros_like(ref_seg) for i in range(3)]\n",
    "refmap_et = ref_seg == 3\n",
    "refmap_tc = np.logical_or(refmap_et, ref_seg == 1)\n",
    "refmap_wt = np.logical_or(refmap_tc, ref_seg == 2)\n",
    "refmap = np.stack([refmap_et, refmap_tc, refmap_wt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fcb1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_metric_list = calculate_metrics(segs[1:], refmap, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686b8da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overlay with Ground seg\n",
    "\n",
    "\n",
    "\n",
    "img = p_image[slice, :, :, :].squeeze()\n",
    "img = utils.make_grid(img)\n",
    "img = img.detach().cpu().numpy()\n",
    "print(img.shape)\n",
    "# plot images\n",
    "plt.figure(figsize=(10, 8))\n",
    "img_list = [img[i].T for i in range(channels)] # 1 image per channel\n",
    "plt.imshow(np.hstack(img_list), cmap='Greys_r')\n",
    "\n",
    "\n",
    "## plot segmentation mask ##\n",
    "seg_img = p_seg_image[slice, :, :, :].squeeze()\n",
    "print(seg_img.shape)\n",
    "seg_img = torch.tensor(seg_img.numpy()[:, ::-1].copy()) #flip\n",
    "seg_img = utils.make_grid(seg_img).detach().cpu().numpy()\n",
    "\n",
    "print(np.unique(p_seg_image))\n",
    "#seg_img = seg_img > 1\n",
    "plt.imshow(np.hstack([seg_img[0].T]), cmap='Greys_r', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e7b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overlay with Predicted\n",
    "img = p_image[slice, :, :, :].squeeze()\n",
    "img = utils.make_grid(img)\n",
    "img = img.detach().cpu().numpy()\n",
    "print(img.shape)\n",
    "# plot images\n",
    "plt.figure(figsize=(10, 8))\n",
    "img_list = [img[i].T for i in range(channels)] # 1 image per channel\n",
    "plt.imshow(np.hstack(img_list), cmap='Greys_r')\n",
    "## plot segmentation mask ##\n",
    "seg_img = torch.tensor(labelmap[slice, :, :].squeeze())\n",
    "seg_img = utils.make_grid(seg_img).detach().cpu().numpy()\n",
    "print(np.unique(seg_img))\n",
    "\n",
    "#plt.figure(figsize=(4, 4))\n",
    "plt.imshow(np.hstack([seg_img[0].T]), cmap='Greys_r', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8644da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
